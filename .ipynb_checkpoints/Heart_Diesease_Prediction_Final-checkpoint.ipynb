{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6982da",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction\n",
    "\n",
    "**Simple notebook following the same STEP 1 / STEP 2... layout**\n",
    "\n",
    "This notebook uses `values.csv` and `labels.csv` (provided) and performs a basic end-to-end workflow: data merge, EDA, preprocessing, model training/comparison, and final conclusion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec79319",
   "metadata": {},
   "source": [
    "## STEP 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd28334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "print('Libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78a4c4",
   "metadata": {},
   "source": [
    "## STEP 2: Load and merge data (same as your file)\n",
    "We will merge `values.csv` and `labels.csv` by row order (concatenate) to reconstruct the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Load and merge data\n",
    "VALUES = 'values.csv'\n",
    "LABELS = 'labels.csv'\n",
    "\n",
    "print('Files in working directory:', os.listdir('.'))\n",
    "\n",
    "if not os.path.exists(VALUES):\n",
    "    raise FileNotFoundError(f\"{VALUES} not found in working directory.\")\n",
    "if not os.path.exists(LABELS):\n",
    "    raise FileNotFoundError(f\"{LABELS} not found in working directory.\")\n",
    "\n",
    "values = pd.read_csv(VALUES)\n",
    "labels = pd.read_csv(LABELS)\n",
    "\n",
    "print('\\nvalues shape:', values.shape)\n",
    "print('labels shape:', labels.shape)\n",
    "\n",
    "# Merge by index (concatenate columns side-by-side)\n",
    "data = pd.concat([values.reset_index(drop=True), labels.reset_index(drop=True)], axis=1)\n",
    "print('merged data shape:', data.shape)\n",
    "\n",
    "# Show first few rows\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79ed4b",
   "metadata": {},
   "source": [
    "## STEP 3: Quick data checks\n",
    "Check for missing values, datatypes, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9de85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Quick data checks\n",
    "print('Columns:', data.columns.tolist())\n",
    "print('\\nData types:\\n', data.dtypes)\n",
    "print('\\nMissing values per column:\\n', data.isnull().sum())\n",
    "\n",
    "# Basic descriptive stats\n",
    "display(data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b662cc",
   "metadata": {},
   "source": [
    "## STEP 4: Exploratory Data Analysis (simple plots)\n",
    "Plots of key numeric features and correlation heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595bed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: EDA plots (simple)\n",
    "# Select numeric columns\n",
    "num_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Plot distributions for some numeric columns (up to 6)\n",
    "for col in num_cols[:6]:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(data[col].dropna(), bins=30)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation heatmap for numeric features\n",
    "if len(num_cols) >= 2:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    corr = data[num_cols].corr()\n",
    "    plt.imshow(corr, aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n",
    "    plt.yticks(range(len(num_cols)), num_cols)\n",
    "    plt.title('Correlation matrix (numeric features)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e61c23",
   "metadata": {},
   "source": [
    "## STEP 5: Prepare data (features X and target y)\n",
    "We will try to detect the target column automatically. If labels had a single column, it will be used as target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Prepare data for modeling\n",
    "# Detect target column in the merged dataframe (common names)\n",
    "possible_targets = ['target','heart_disease_present','label','diagnosis','has_disease','disease']\n",
    "target = None\n",
    "for t in possible_targets:\n",
    "    if t in data.columns:\n",
    "        target = t\n",
    "        break\n",
    "# If not found and labels had exactly 1 column, take that column\n",
    "if target is None:\n",
    "    if labels.shape[1] == 1:\n",
    "        target = labels.columns[0]\n",
    "    else:\n",
    "        # fallback: last column\n",
    "        target = data.columns[-1]\n",
    "\n",
    "print('Using target column:', target)\n",
    "X = data.drop(columns=[target])\n",
    "y = data[target]\n",
    "\n",
    "print('Feature shape:', X.shape, 'Target shape:', y.shape)\n",
    "print('Target distribution:\\n', y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb279a",
   "metadata": {},
   "source": [
    "## STEP 6: Preprocessing\n",
    "Simple imputation for numeric and one-hot for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35519acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Preprocessing setup\n",
    "# Identify numeric and categorical features\n",
    "num_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "# Some integer-coded categoricals might appear in num_features; keep it simple as-is\n",
    "print('Numeric features:', num_features)\n",
    "print('Categorical features:', cat_features)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_features),\n",
    "    ('cat', categorical_transformer, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a94061",
   "metadata": {},
   "source": [
    "## STEP 7: Train/test split and Model training\n",
    "We train Logistic Regression, Random Forest, SVM, and KNN. We'll compare results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print('\\nTraining', name)\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('clf', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    # try to get probability for ROC AUC\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = pipe.predict_proba(X_test)[:,1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba) if y_proba is not None and len(y_test.unique())>1 else None\n",
    "    print('Accuracy:', acc)\n",
    "    print('Precision:', prec)\n",
    "    print('Recall:', rec)\n",
    "    print('F1:', f1)\n",
    "    if auc is not None:\n",
    "        print('ROC AUC:', auc)\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    results.append({'model': name, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'roc_auc': auc})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by='f1', ascending=False).reset_index(drop=True)\n",
    "print('\\nModel comparison:')\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adbd97",
   "metadata": {},
   "source": [
    "## STEP 8: Conclusion and save best model\n",
    "We save the best model (by F1 score) to `best_model.joblib` and provide a short recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Save best model and conclusion\n",
    "if not results_df.empty:\n",
    "    best = results_df.iloc[0]['model']\n",
    "    print('Best model by F1:', best)\n",
    "    # instantiate the best model class again\n",
    "    model_inst = None\n",
    "    if best == 'LogisticRegression':\n",
    "        model_inst = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    elif best == 'RandomForest':\n",
    "        model_inst = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    elif best == 'SVM':\n",
    "        model_inst = SVC(probability=True, random_state=42)\n",
    "    elif best == 'KNN':\n",
    "        model_inst = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    if model_inst is not None:\n",
    "        best_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('clf', model_inst)])\n",
    "        # retrain on full data\n",
    "        best_pipe.fit(pd.concat([X_train, X_test]), pd.concat([y_train, y_test]))\n",
    "        joblib.dump(best_pipe, 'best_model.joblib')\n",
    "        print('Saved best model pipeline to best_model.joblib')\n",
    "\n",
    "# Short recommendation\n",
    "print('\\nRecommendation:')\n",
    "print('- Use the best model to screen patients and flag high-risk cases.')\n",
    "print('- Collect more labeled data and add domain-specific features for better performance.')\n",
    "print('- For production, wrap the pipeline with input validation and monitoring.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
