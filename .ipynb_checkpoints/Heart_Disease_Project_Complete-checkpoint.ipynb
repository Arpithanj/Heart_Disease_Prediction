{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4cfccf9",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction â€” Complete Project\n",
    "\n",
    "Author: Generated for you\n",
    "\n",
    "This notebook contains a full project workflow aligned to your project brief:\n",
    "- Data loading & cleaning\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Feature engineering & preprocessing\n",
    "- Model training & comparison (Logistic Regression, Random Forest, SVM, KNN, XGBoost)\n",
    "- Model selection and export\n",
    "- Recommendations for hospital use and discussion of challenges\n",
    "\n",
    "You can run this notebook end-to-end. It expects `values.csv` and `labels.csv` to be in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and loading data\n",
    "import os, sys\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "# Optional: XGBoost (if installed)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    has_xgb = True\n",
    "except Exception:\n",
    "    has_xgb = False\n",
    "\n",
    "print('xgboost available:', has_xgb)\n",
    "\n",
    "# Load files\n",
    "VALUES = 'values.csv'\n",
    "LABELS = 'labels.csv'\n",
    "\n",
    "for f in [VALUES, LABELS]:\n",
    "    print('Exists', f, os.path.exists(f))\n",
    "\n",
    "df_values = pd.read_csv(VALUES)\n",
    "df_labels = pd.read_csv(LABELS)\n",
    "\n",
    "print('\\nvalues.csv shape:', df_values.shape)\n",
    "print('labels.csv shape:', df_labels.shape)\n",
    "\n",
    "# Merge on patient id if possible\n",
    "# common ids columns: try to detect a common column name\n",
    "common_cols = set(df_values.columns).intersection(df_labels.columns)\n",
    "if common_cols:\n",
    "    key = list(common_cols)[0]\n",
    "    print('Merging on', key)\n",
    "    df = df_values.merge(df_labels, on=key)\n",
    "else:\n",
    "    # if labels file only has a single column 'target' or 'label', try concat by index\n",
    "    if df_labels.shape[1] == 1:\n",
    "        label_col = df_labels.columns[0]\n",
    "        df = df_values.copy()\n",
    "        df[label_col] = df_labels[label_col]\n",
    "        print('Appended labels column as', label_col)\n",
    "    else:\n",
    "        # fallback to concat columns side-by-side\n",
    "        df = pd.concat([df_values.reset_index(drop=True), df_labels.reset_index(drop=True)], axis=1)\n",
    "        print('Concatenated values and labels')\n",
    "\n",
    "print('Merged df shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9cef0",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section performs basic EDA: missing values, distributions, and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1909c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: missing values and distributions\n",
    "print('\\nMissing values per column:\\n', df.isnull().sum())\n",
    "\n",
    "# Basic stats\n",
    "display(df.describe(include='all'))\n",
    "\n",
    "# Plot distribution for numeric columns (first 8 to avoid overcrowding)\n",
    "num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "print('Numeric columns:', num_cols)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "for col in num_cols[:8]:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(df[col].dropna(), bins=30)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation matrix (numeric features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbafb5",
   "metadata": {},
   "source": [
    "## Preprocessing & Feature Engineering\n",
    "\n",
    "- Handle missing values\n",
    "- Encode categorical variables\n",
    "- Scale numeric features\n",
    "- Prepare X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25812c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns and target\n",
    "# Try to detect a target column automatically (common names)\n",
    "possible_targets = ['target','heart_disease_present','label','diagnosis','has_disease','disease']\n",
    "target = None\n",
    "for t in possible_targets:\n",
    "    if t in df.columns:\n",
    "        target = t\n",
    "        break\n",
    "if target is None:\n",
    "    # fallback: assume last column is target\n",
    "    target = df.columns[-1]\n",
    "print('Using target column:', target)\n",
    "\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "num_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "# Some integer-coded categorical columns: check cardinality\n",
    "for col in num_features[:]:\n",
    "    if X[col].nunique() < 10 and col not in ['age','resting_blood_pressure','serum_cholesterol_mg_per_dl','max_heart_rate_achieved']:\n",
    "        # treat as categorical\n",
    "        cat_features.append(col)\n",
    "        num_features.remove(col)\n",
    "\n",
    "print('Numeric features:', num_features)\n",
    "print('Categorical features:', cat_features)\n",
    "\n",
    "# Build preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_features),\n",
    "    ('cat', categorical_transformer, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8336ab",
   "metadata": {},
   "source": [
    "## Model Training & Comparison\n",
    "We'll train several models and compare performance using cross-validation and a hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f64ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (stratified if possible)\n",
    "from sklearn.model_selection import train_test_split\n",
    "if len(y.unique()) > 1:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'SVC': SVC(probability=True, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "if has_xgb:\n",
    "    models['XGBoost'] = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test, preprocessor):\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('clf', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = pipe.predict_proba(X_test)[:,1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba) if y_proba is not None and len(np.unique(y_test))>1 else None\n",
    "    print(f'== {name} ==')\n",
    "    print('Accuracy:', acc)\n",
    "    print('Precision:', prec)\n",
    "    print('Recall:', rec)\n",
    "    print('F1:', f1)\n",
    "    if auc is not None:\n",
    "        print('ROC AUC:', auc)\n",
    "    print('\\nClassification report:\\n', classification_report(y_test, y_pred, zero_division=0))\n",
    "    print('\\nConfusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    return dict(name=name, accuracy=acc, precision=prec, recall=rec, f1=f1, roc_auc=auc)\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        res = evaluate_model(name, model, X_train, X_test, y_train, y_test, preprocessor)\n",
    "        results.append(res)\n",
    "    except Exception as e:\n",
    "        print('Error training', name, e)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by='f1', ascending=False)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090df76",
   "metadata": {},
   "source": [
    "## Save Best Model\n",
    "We'll save the best model pipeline to `best_model.joblib` for later deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1798f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    best_name = results_df.iloc[0]['name']\n",
    "    print('Best model by F1:', best_name)\n",
    "    best_model = None\n",
    "    # retrain best model on full data (train+test)\n",
    "    model_instance = None\n",
    "    if best_name == 'LogisticRegression':\n",
    "        model_instance = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    elif best_name == 'RandomForest':\n",
    "        model_instance = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    elif best_name == 'SVC':\n",
    "        model_instance = SVC(probability=True, random_state=42)\n",
    "    elif best_name == 'KNN':\n",
    "        model_instance = KNeighborsClassifier(n_neighbors=5)\n",
    "    elif best_name == 'XGBoost' and has_xgb:\n",
    "        model_instance = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    if model_instance is not None:\n",
    "        full_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('clf', model_instance)])\n",
    "        full_pipe.fit(pd.concat([X_train, X_test]), pd.concat([y_train, y_test]))\n",
    "        joblib.dump(full_pipe, 'best_model.joblib')\n",
    "        print('Saved best model pipeline to best_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748c0a6",
   "metadata": {},
   "source": [
    "## Recommendations for Hospital\n",
    "\n",
    "- Integrate the saved model into screening workflows to flag high-risk patients.\n",
    "- Collect more labeled data, especially for underrepresented groups.\n",
    "- Add follow-up and outcome data to improve temporal predictions.\n",
    "- Use explainability (SHAP or LIME) to provide clinicians with reasons for predictions.\n",
    "\n",
    "## Challenges Faced\n",
    "\n",
    "- Missing values and inconsistent column naming across files.\n",
    "- Potential class imbalance in heart disease labels (use stratified sampling).\n",
    "- Categorical variables encoded as integers needing domain knowledge.\n",
    "- Need to avoid data leakage by proper pipeline usage.\n",
    "\n",
    "---\n",
    "\n",
    "## How to run\n",
    "1. Place `values.csv` and `labels.csv` in the same directory as this notebook.\n",
    "2. Run cells top to bottom. `xgboost` is optional but recommended if available.\n",
    "3. The notebook will produce evaluation metrics and save `best_model.joblib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example after running and saving model\n",
    "if os.path.exists('best_model.joblib'):\n",
    "    pipe = joblib.load('best_model.joblib')\n",
    "    # take first row (drop target if present)\n",
    "    sample = df.drop(columns=[target]).iloc[0:2]\n",
    "    print('Sample input:\\n', sample)\n",
    "    preds = pipe.predict(sample)\n",
    "    probs = None\n",
    "    try:\n",
    "        probs = pipe.predict_proba(sample)[:,1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    print('Predictions:', preds)\n",
    "    if probs is not None:\n",
    "        print('Probabilities:', probs)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
